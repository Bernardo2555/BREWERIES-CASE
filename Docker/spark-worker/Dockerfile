# FROM openjdk:17-slim

# USER root

# # üîß Instala depend√™ncias b√°sicas
# RUN apt-get update && apt-get install -y \
#     python3 \
#     python3-pip \
#     curl \
#     procps \
#     && apt-get clean && rm -rf /var/lib/apt/lists/*

# # ‚öôÔ∏è Vari√°veis de ambiente
# ENV JAVA_HOME="/usr/local/openjdk-17"
# ENV SPARK_HOME="/opt/spark"
# ENV PATH="${JAVA_HOME}/bin:${SPARK_HOME}/bin:${SPARK_HOME}/sbin:${PATH}"

# # üöÄ Instala Apache Spark 4.0.1
# RUN mkdir -p ${SPARK_HOME} && \
#     curl -fSL https://downloads.apache.org/spark/spark-4.0.1/spark-4.0.1-bin-hadoop3.tgz -o /tmp/spark.tgz && \
#     tar -xzf /tmp/spark.tgz -C ${SPARK_HOME} --strip-components=1 && \
#     rm /tmp/spark.tgz

# # üêç Instala PySpark e Delta Lake
# RUN pip install --upgrade pip && pip install --no-cache-dir pyspark==4.0.1 delta-spark

# # üîê Ajusta permiss√µes
# RUN chown -R 1000:1000 ${SPARK_HOME}
# USER 1000

FROM eclipse-temurin:17-jdk

USER root

# Define vari√°veis
ENV SPARK_VERSION=4.0.1
ENV HADOOP_VERSION=3
ENV SPARK_HOME=/opt/spark
ENV PATH="$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH"
ENV PYSPARK_PYTHON=python3
ENV PYSPARK_DRIVER_PYTHON=python3

# Instala depend√™ncias
RUN apt-get update && apt-get install -y wget curl bash procps python3 python3-pip && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

# Copia o Spark j√° extra√≠do para o container
COPY spark-4.0.1-bin-hadoop3/ /opt/spark
COPY spark-defaults.conf /opt/spark/conf/spark-defaults.conf
COPY log4j2.properties /opt/spark/conf/log4j2.properties
RUN chmod -R 755 /opt/spark

# Inicia o Spark Master
CMD ["/opt/spark/sbin/start-worker.sh", "spark://spark-master:7077"]